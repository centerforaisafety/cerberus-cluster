{% set controller = hostvars[groups['controller'][0]]['ansible_fqdn'].split('.') %}
SlurmctldHost={{ controller[0] }}
{% if (groups['slurm_backup']| length ) > 0 %}
SlurmctldHost={{ hostvars[groups['slurm_backup'][0]]['ansible_fqdn'].split('.')[0] }}
{% endif %}
MpiDefault=none
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
{% if pyxis|bool or healthchecks|bool%}
Prolog={{slurm_conf_path}}/prolog.d/*
SchedulerParameters=nohold_on_prolog_fail
{% endif %}
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
StateSaveLocation={{ slurm_nfs_path }}/spool/slurm
SwitchType=switch/none
TaskPlugin=task/affinity,task/cgroup
JobContainerType=job_container/tmpfs
PrologFlags=contain
InactiveLimit=0
KillWait=120
MaxJobCount=100000
MinJobAge=21600
SlurmctldTimeout=300
SlurmdTimeout=600
Waittime=0
GresTypes=gpu
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
AccountingStorageHost={{ controller[0] }}
AccountingStorageType=accounting_storage/slurmdbd
TaskPlugin=task/affinity,task/cgroup
AccountingStoreFlags=job_comment
ClusterName=cluster
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=info
SlurmdDebug=info
EnforcePartLimits=ALL
# Default Memory Per GPU in MB
DefMemPerGPU=200000
# Give 2 CPUs per GPU
DefCpuPerGPU=2
PropagateResourceLimitsExcept=MEMLOCK
CommunicationParameters=block_null_hash
TopologyPlugin=topology/tree
TopologyParam=SwitchAsNodeRank
TreeWidth=2048
SlurmctldParameters=enable_configless
MaxNodeCount=10000
{% if healthchecks|bool %}
HealthCheckProgram={{slurm_conf_path}}/prolog.d/healthchecks.sh
HealthCheckInterval=300
HealthCheckNodeState=IDLE,CYCLE
{% endif %}
MailProg=/bin/goslmailer
UnkillableStepTimeout=120

# Priority/Fairshare section
PriorityType=priority/multifactor
# 2 week half-life
PriorityDecayHalfLife=14-0
# The larger the job, the greater its job size priority.
PriorityFavorSmall=no

# FairShareDampeningFactor=2

# The job's age factor reaches 1.0 after waiting in the
# queue for 2 weeks.
PriorityMaxAge=14-0

# This next group determines the weighting of each of the
# components of the Multi-factor Job Priority Plugin.
# The default value for each of the following is 1.
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000

{% if sacct_limits|bool %}
AccountingStorageTRES=gres/gpu,gres/gpu:A100
AccountingStorageEnforce=limits,associations,qos,safe
JobCompType=jobcomp/none
TrackWckey=no
{% endif %}

{% for partition in queues %}
{% for instance in partition.instance_types %}
Nodeset={{instance.name}} Feature={{instance.name}}
{% endfor %}
{% endfor %}

{% for partition in queues %}
{% if partition.default  %}
{% set nodesList = [] %}
{% for instance in partition.instance_types %}
{{ nodesList.append(instance.name)}}
{%- endfor %}
PartitionName={{partition.name}} Nodes={{nodesList|join(',')}} Default=YES DefaultTime=02:00:00 MaxTime=2-0 State=UP
{% else %}
{% set nodesList = [] %}
{% for instance in partition.instance_types %}
{{ nodesList.append(instance.name)}}
{%- endfor %}
PartitionName={{partition.name}} Nodes={{nodesList|join(',')}} Default=NO
{% endif %}
{% endfor %}